{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB_ESC2vFhpv"
      },
      "source": [
        "# Assignment B.5 - Modern face recognition with deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4Wt73GhGH-M"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O56jp96I0MQo"
      },
      "source": [
        "Have you noticed that Facebook has developed an uncanny ability to recognize your friends in your photographs? In the old days, Facebook used to make you to tag your friends in photos by clicking on them and typing in their name. Now as soon as you upload a photo, Facebook tags everyone for you like magic. This technology is called face recognition. Facebook’s algorithms are able to recognize your friends’ faces after they have been tagged only a few times. It’s pretty amazing: these algorithms can recognize faces with 98% accuracy, which is pretty much as good as humans can do!\n",
        "\n",
        "As a human, your brain is wired to recognize faces automatically and instantly. Computers are not capable of doing this, so you have to teach them how to tackle each step in this process. Specifically, a face recognition system goes through four steps: find faces in the image, analyze their facial features, compare against known faces, and make a prediction of the corresponding persons. Here's described the full pipeline.\n",
        "\n",
        "<img src=\"https://perso.esiee.fr/~najmanl/FaceRecognition/figures/summary.gif\" style=\"height:200px;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B13bOfafF0Zr"
      },
      "source": [
        "## Table of contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qitLYFkhF6Zu"
      },
      "source": [
        "In this assignment, you will tackle several problems related to face recognition:\n",
        "1. **Face detection**. Look at a picture and find all the faces in it. -5\n",
        "2. **Pose estimation**. Understand where the face is turned and correct its pose. 5\n",
        "3. **Face encoding**. Pick up unique features from a face that can be used to distinguish it from others. 5\n",
        "4. **Face recognition**. Compare the unique features of a face to those of all the people in a database. 5\n",
        "5. **Personal dataset**. Build a custom face recognition dataset. 2\n",
        "\n",
        "By the end of this notebook, you will have your own face recognition system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxS9du380MQw"
      },
      "source": [
        "## Required packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rzb4cMM-GBAG"
      },
      "source": [
        "Here are the packages you will need during the assignment.\n",
        "- [Numpy](http://www.numpy.org)\n",
        "- [Keras](https://keras.io)\n",
        "- [OpenCV](https://opencv.org)\n",
        "- [Dlib](http://dlib.net).\n",
        "\n",
        "**Note**: In Anaconda Navigator, the package `dlib` can be installed from the **conda-forge** channel. In Google Colab, everything is readily available"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9B6243DPGDQn"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cy3onABl0MQ0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "import cv2\n",
        "import dlib\n",
        "import os\n",
        "import keras\n",
        "import sklearn\n",
        "import urllib.request\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o09mxhe1GMWW"
      },
      "source": [
        "## 1. Face detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnCc8P7y0MQ1"
      },
      "source": [
        "The first step in your pipeline is face detection. Obviously you need to locate the faces in a photograph before you can try to tell them apart. If you’ve used any camera in the last 10 years, you’ve probably seen face detection in action. Face detection is a great feature for cameras. When the camera can automatically pick out faces, it can make sure that all the faces are in focus before it takes the picture. But you’ll use it for a different purpose: finding the areas of the image you want to pass on to the next step in your pipeline.\n",
        "\n",
        "<img src=\"https://perso.esiee.fr/~najmanl/FaceRecognition/figures/detection.jpg\" style=\"height:200px;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEHKcX4fHznY"
      },
      "source": [
        "### Assignement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A5esp2s0MQ2"
      },
      "source": [
        "Here's what you are required to do for this part of the assignment.\n",
        "\n",
        "- You are provided with a small dataset of pictures, where each picture contains exactly one face. Extract the faces and their labels (i.e., the person's names). Store them to a new file with the function `dump` in the package `pickle`.\n",
        "\n",
        "\n",
        "-  Normalize the cropped faces (i.e., divide the pixel values by 255), and split them in train set (70%) and test set (30%) with the function `train_test_split` in the package `sklearn`.\n",
        "\n",
        "\n",
        "- Train a small convnet and check its performance on the test set. Remember: don't use the test images for training.\n",
        "\n",
        "\n",
        "- Try to improve the performance of the baseline convnet by using all the tricks you have learned in the course."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4U0Ls78qH4xJ"
      },
      "source": [
        "### Provided functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfqaWaKR0MQ3"
      },
      "source": [
        "Here you will find some useful functions to complete the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEf_l6VLXTXe"
      },
      "outputs": [],
      "source": [
        "def download_zip(url, file_name):\n",
        "    urllib.request.urlretrieve(url, file_name)\n",
        "\n",
        "    with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "\n",
        "    os.remove(file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79wTlPUP2SDU"
      },
      "outputs": [],
      "source": [
        "# !wget https://perso.esiee.fr/~najmanl/FaceRecognition/models.zip\n",
        "# !unzip models.zip\n",
        "download_zip('https://perso.esiee.fr/~najmanl/FaceRecognition/models.zip', 'models.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "collapsed": true,
        "id": "0RW7kXCj0MQ4",
        "outputId": "0bcb25f5-18a8-4b52-d150-99f141d59750"
      },
      "outputs": [],
      "source": [
        "hog_detector = dlib.get_frontal_face_detector()\n",
        "cnn_detector = dlib.cnn_face_detection_model_v1('models/mmod_human_face_detector.dat')\n",
        "\n",
        "def face_locations(image, model=\"hog\"):\n",
        "\n",
        "    if model == \"hog\":\n",
        "        detector = hog_detector\n",
        "        cst = 0\n",
        "    elif model == \"cnn\":\n",
        "        detector = cnn_detector\n",
        "        cst = 10\n",
        "\n",
        "    matches = detector(image,1)\n",
        "    rects   = []\n",
        "\n",
        "    for r in matches:\n",
        "        if model == \"cnn\":\n",
        "            r = r.rect\n",
        "        x = max(r.left(), 0)\n",
        "        y = max(r.top(), 0)\n",
        "        w = min(r.right(), image.shape[1]) - x + cst\n",
        "        h = min(r.bottom(), image.shape[0]) - y + cst\n",
        "        rects.append((x,y,w,h))\n",
        "\n",
        "    return rects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "S4HSi9Qc0MQ6"
      },
      "outputs": [],
      "source": [
        "def extract_faces(image, model=\"hog\"):\n",
        "\n",
        "    gray  = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "    rects = face_locations(gray, model)\n",
        "    faces = []\n",
        "\n",
        "    for (x,y,w,h) in rects:\n",
        "        cropped = image[y:y+h, x:x+w, :]\n",
        "        cropped = cv2.resize(cropped, (128,128))\n",
        "        faces.append(cropped)\n",
        "\n",
        "    return faces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "k7v9sVPa0MQ7"
      },
      "outputs": [],
      "source": [
        "def show_grid(faces, figsize=(12,3)):\n",
        "\n",
        "    n = len(faces)\n",
        "    cols = 7\n",
        "    rows = int(np.ceil(n/cols))\n",
        "\n",
        "    fig, ax = plt.subplots(rows,cols, figsize=figsize)\n",
        "\n",
        "    for r in range(rows):\n",
        "        for c in range(cols):\n",
        "            i = r*cols + c\n",
        "            if i == n:\n",
        "                 break\n",
        "            ax[r,c].imshow(faces[i])\n",
        "            ax[r,c].axis('off')\n",
        "            #ax[r,c].set_title('size: ' + str(faces[i].shape[:2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UDMX9I2Y0MQ7"
      },
      "outputs": [],
      "source": [
        "def list_images(basePath, validExts=(\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"), contains=None):\n",
        "\n",
        "    imagePaths = []\n",
        "\n",
        "    # loop over the directory structure\n",
        "    for (rootDir, dirNames, filenames) in os.walk(basePath):\n",
        "        # loop over the filenames in the current directory\n",
        "        for filename in filenames:\n",
        "            # if the contains string is not none and the filename does not contain\n",
        "            # the supplied string, then ignore the file\n",
        "            if contains is not None and filename.find(contains) == -1:\n",
        "                continue\n",
        "\n",
        "            # determine the file extension of the current file\n",
        "            ext = filename[filename.rfind(\".\"):].lower()\n",
        "\n",
        "            # check to see if the file is an image and should be processed\n",
        "            if ext.endswith(validExts):\n",
        "                # construct the path to the image and yield it\n",
        "                imagePath = os.path.join(rootDir, filename).replace(\" \", \"\\\\ \")\n",
        "                imagePaths.append(imagePath)\n",
        "\n",
        "    return imagePaths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urvgtd1MH9oJ"
      },
      "source": [
        "### Hints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMuqxVrJ0MQ9"
      },
      "source": [
        "The provided function `extract_faces()` applies face detection to a single input image, and returns a list of 128x128 blocks containing the detected faces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybpSOApP29TS"
      },
      "outputs": [],
      "source": [
        "# !wget https://perso.esiee.fr/~najmanl/FaceRecognition/figures.zip\n",
        "# !unzip figures.zip\n",
        "download_zip('https://perso.esiee.fr/~najmanl/FaceRecognition/figures.zip', 'figures.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fi1aVAyp0MQ9"
      },
      "outputs": [],
      "source": [
        "image = cv2.imread(\"figures/faces.png\")\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AJtMFU3W0MQ-"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "plt.imshow(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UwDDUNJ60MQ-"
      },
      "outputs": [],
      "source": [
        "faces = extract_faces(image, \"cnn\")  # Replace 'cnn' with 'hog' for faster but less accurate results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EeJIoFF20MQ_"
      },
      "outputs": [],
      "source": [
        "show_grid(faces)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfp7kZK10MQ_"
      },
      "source": [
        "Moreover, the function `list_images()` locates all the jpeg/png/tiff files in a given folder (including its subfolders)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VE-KvzQv3To6"
      },
      "outputs": [],
      "source": [
        "# !wget https://perso.esiee.fr/~najmanl/FaceRecognition/data.zip\n",
        "# !unzip data.zip\n",
        "download_zip('https://perso.esiee.fr/~najmanl/FaceRecognition/data.zip', 'data.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Qkj4VLbT0MRA"
      },
      "outputs": [],
      "source": [
        "imagePaths = list_images(\"data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bSyXSoMIGLQ"
      },
      "source": [
        "### Answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ9ghiQGbeG8"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import timeit\n",
        "\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, SeparableConv2D\n",
        "from keras.regularizers import l2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmbVI7HRWiOb"
      },
      "outputs": [],
      "source": [
        "def img_labels_faces(img_paths_list):\n",
        "  labels_faces = np.empty((0, 2), dtype=object)\n",
        "  for img_path in img_paths_list:\n",
        "    try:\n",
        "      img = cv2.imread(img_path)\n",
        "      img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "      face = extract_faces(img, \"cnn\")\n",
        "      label = img_path.split('/')[-2]\n",
        "      face = np.array(face).reshape(128, 128, 3)\n",
        "      labels_faces = np.vstack((labels_faces, np.array([label, np.array(face)])))\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "\n",
        "  return labels_faces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dHgRzVvZFW1"
      },
      "outputs": [],
      "source": [
        "def load_data(data_path_str):\n",
        "  try :\n",
        "    with open(data_path_str, \"rb\") as file:\n",
        "      data = pickle.load(file)\n",
        "    return data\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "  return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8n-AMSfca0cP"
      },
      "outputs": [],
      "source": [
        "def save_data(data, data_path_str):\n",
        "  try :\n",
        "    with open(data_path_str, \"wb\") as file:\n",
        "      pickle.dump(data, file)\n",
        "  except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIplIqLovvqX"
      },
      "outputs": [],
      "source": [
        "def labels_to_int(labels):\n",
        "  le = LabelEncoder().fit(labels)\n",
        "  return le.transform(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEj3JTmogycv"
      },
      "outputs": [],
      "source": [
        "def labels_to_categorical(labels):\n",
        "  return to_categorical(labels_to_int(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1C3F6bAjjtDL"
      },
      "outputs": [],
      "source": [
        "def plot_model_history(history):\n",
        "  acc = history.history['accuracy']\n",
        "  val_acc = history.history['val_accuracy']\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  epochs = range(len(acc))\n",
        "\n",
        "  plt.plot(epochs, acc, 'b', label='Training acc')\n",
        "  plt.plot(epochs, val_acc, 'm', label='Validation acc')\n",
        "  plt.title('Training and validation accuracy')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.figure()\n",
        "\n",
        "  plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "  plt.plot(epochs, val_loss, 'm', label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtnrtXGx1k5d"
      },
      "outputs": [],
      "source": [
        "def cnn_7(train_dataset, test_dataset, epochs=100):\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(SeparableConv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(128, 128, 3)))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(64, activation='relu'))\n",
        "  model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  history = model.fit(train_dataset, epochs=epochs, validation_data=test_dataset, verbose=0)\n",
        "\n",
        "  return history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "520JDqhT3UB_"
      },
      "outputs": [],
      "source": [
        "def cnn_12(train_dataset, test_dataset, epochs=100):\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(128, 128, 3)))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(512, activation='relu'))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(1024, kernel_regularizer=l2(0.001), activation='relu'))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(512, activation='relu'))\n",
        "  model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  history = model.fit(train_dataset, epochs=epochs, validation_data=test_dataset, verbose=0)\n",
        "\n",
        "  return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__IYXAYsQhiD"
      },
      "outputs": [],
      "source": [
        "download_zip(\"https://perso.esiee.fr/~najmanl/FaceRecognition/data.zip\", \"data.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbsBeDnIQgyY"
      },
      "outputs": [],
      "source": [
        "img_paths_list = list_images(\"data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zv2Jb8vBdfY0"
      },
      "outputs": [],
      "source": [
        "img_paths_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiv7tNGiXTXh"
      },
      "outputs": [],
      "source": [
        "labels_faces = img_labels_faces(img_paths_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsrPXIaOeBee"
      },
      "outputs": [],
      "source": [
        "save_data(labels_faces, \"labels_faces\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwG5Ck53QvCw"
      },
      "outputs": [],
      "source": [
        "labels = np.array([data[0] for data in labels_faces])\n",
        "faces = np.array([data[1] for data in labels_faces])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dY6t8ySNfTH9"
      },
      "outputs": [],
      "source": [
        "labels_one_hot = labels_to_categorical(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zanF1fJkQxyS"
      },
      "outputs": [],
      "source": [
        "faces_train, faces_test, labels_train, labels_test = train_test_split(\n",
        "        faces,\n",
        "        labels_one_hot,\n",
        "        test_size=0.3,\n",
        "        random_state=42\n",
        "        )\n",
        "\n",
        "faces_train_normalized = np.array(faces_train) / 255.0\n",
        "faces_test_normalized = np.array(faces_test) / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZ9iMGdXRJZH"
      },
      "outputs": [],
      "source": [
        "train_data_augmentation = image.ImageDataGenerator(\n",
        "        rescale=1. / 255,\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "train_dataset = train_data_augmentation.flow(\n",
        "        faces_train,\n",
        "        labels_train,\n",
        "        batch_size=15,\n",
        "        ignore_class_split=True,\n",
        "        subset='training'\n",
        "    )\n",
        "\n",
        "test_data_augmentation = image.ImageDataGenerator(rescale=1. / 255)\n",
        "test_dataset = test_data_augmentation.flow(\n",
        "        faces_test,\n",
        "        labels_test,\n",
        "        batch_size=15,\n",
        "        ignore_class_split=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBGPdYxSRKMv"
      },
      "outputs": [],
      "source": [
        "plot_model_history(cnn_7(train_dataset, test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJZwCK4iRRMm"
      },
      "outputs": [],
      "source": [
        "plot_model_history(cnn_12(train_dataset, test_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3WLheP_GRXo"
      },
      "source": [
        "## 2. Pose estimation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFrGlU2q0MRB"
      },
      "source": [
        "You have isolated the faces in our image. But now you have to deal with the problem that faces turned different directions look totally different to a computer. To account for this, you will try to warp each picture so that the eyes and lips are always in the same place in the image. More concretely, you are going to use an algorithm called face landmark estimation. The basic idea is to locate 68 specific points (called landmarks) that exist on every face:  the top of the chin, the outside edge of each eye, the inner edge of each eyebrow, etc. Then, you’ll simply rotate, scale and shear the image so that the eyes and mouth are centered as best as possible. This will make face recognition more accurate.\n",
        "\n",
        "<img src=\"https://perso.esiee.fr/~najmanl/FaceRecognition/figures/pose.png\" style=\"height:200px;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKzplr10ITpI"
      },
      "source": [
        "### Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLBTOO7F0MRB"
      },
      "source": [
        "Here's what you are required to do for this part of the assignment.\n",
        "\n",
        "- Further preprocess the face pictures by correcting their pose. You should now have a dataset of cropped, aligned, and normalized faces.\n",
        "\n",
        "\n",
        "- Re-train your convnets on the modified dataset.\n",
        "\n",
        "\n",
        "- Evaluate the performance on the test set, and compare it to the scores obtained with your previously trained convnets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-62R6B0KIst5"
      },
      "source": [
        "### Provided functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEeUbQSn0MRB"
      },
      "source": [
        "Here you will find some useful functions to complete the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BgIWJ13c0MRB"
      },
      "outputs": [],
      "source": [
        "pose68 = dlib.shape_predictor('models/shape_predictor_68_face_landmarks.dat')\n",
        "pose05 = dlib.shape_predictor('models/shape_predictor_5_face_landmarks.dat')\n",
        "\n",
        "def face_landmarks(face, model=\"large\"):\n",
        "\n",
        "    if model == \"large\":\n",
        "        predictor = pose68\n",
        "    elif model == \"small\":\n",
        "        predictor = pose05\n",
        "\n",
        "    if not isinstance(face, list):\n",
        "        rect = dlib.rectangle(0,0,face.shape[1],face.shape[0])\n",
        "        return predictor(face, rect)\n",
        "    else:\n",
        "        rect = dlib.rectangle(0,0,face[0].shape[1],face[0].shape[0])\n",
        "        return [predictor(f,rect) for f in face]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zYW708ym0MRB"
      },
      "outputs": [],
      "source": [
        "def shape_to_coords(shape):\n",
        "    return np.float32([[p.x, p.y] for p in shape.parts()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0n4wwxOG0MRC"
      },
      "outputs": [],
      "source": [
        "TEMPLATE = np.float32([\n",
        "    (0.0792396913815, 0.339223741112), (0.0829219487236, 0.456955367943),\n",
        "    (0.0967927109165, 0.575648016728), (0.122141515615, 0.691921601066),\n",
        "    (0.168687863544, 0.800341263616), (0.239789390707, 0.895732504778),\n",
        "    (0.325662452515, 0.977068762493), (0.422318282013, 1.04329000149),\n",
        "    (0.531777802068, 1.06080371126), (0.641296298053, 1.03981924107),\n",
        "    (0.738105872266, 0.972268833998), (0.824444363295, 0.889624082279),\n",
        "    (0.894792677532, 0.792494155836), (0.939395486253, 0.681546643421),\n",
        "    (0.96111933829, 0.562238253072), (0.970579841181, 0.441758925744),\n",
        "    (0.971193274221, 0.322118743967), (0.163846223133, 0.249151738053),\n",
        "    (0.21780354657, 0.204255863861), (0.291299351124, 0.192367318323),\n",
        "    (0.367460241458, 0.203582210627), (0.4392945113, 0.233135599851),\n",
        "    (0.586445962425, 0.228141644834), (0.660152671635, 0.195923841854),\n",
        "    (0.737466449096, 0.182360984545), (0.813236546239, 0.192828009114),\n",
        "    (0.8707571886, 0.235293377042), (0.51534533827, 0.31863546193),\n",
        "    (0.516221448289, 0.396200446263), (0.517118861835, 0.473797687758),\n",
        "    (0.51816430343, 0.553157797772), (0.433701156035, 0.604054457668),\n",
        "    (0.475501237769, 0.62076344024), (0.520712933176, 0.634268222208),\n",
        "    (0.565874114041, 0.618796581487), (0.607054002672, 0.60157671656),\n",
        "    (0.252418718401, 0.331052263829), (0.298663015648, 0.302646354002),\n",
        "    (0.355749724218, 0.303020650651), (0.403718978315, 0.33867711083),\n",
        "    (0.352507175597, 0.349987615384), (0.296791759886, 0.350478978225),\n",
        "    (0.631326076346, 0.334136672344), (0.679073381078, 0.29645404267),\n",
        "    (0.73597236153, 0.294721285802), (0.782865376271, 0.321305281656),\n",
        "    (0.740312274764, 0.341849376713), (0.68499850091, 0.343734332172),\n",
        "    (0.353167761422, 0.746189164237), (0.414587777921, 0.719053835073),\n",
        "    (0.477677654595, 0.706835892494), (0.522732900812, 0.717092275768),\n",
        "    (0.569832064287, 0.705414478982), (0.635195811927, 0.71565572516),\n",
        "    (0.69951672331, 0.739419187253), (0.639447159575, 0.805236879972),\n",
        "    (0.576410514055, 0.835436670169), (0.525398405766, 0.841706377792),\n",
        "    (0.47641545769, 0.837505914975), (0.41379548902, 0.810045601727),\n",
        "    (0.380084785646, 0.749979603086), (0.477955996282, 0.74513234612),\n",
        "    (0.523389793327, 0.748924302636), (0.571057789237, 0.74332894691),\n",
        "    (0.672409137852, 0.744177032192), (0.572539621444, 0.776609286626),\n",
        "    (0.5240106503, 0.783370783245), (0.477561227414, 0.778476346951)])\n",
        "\n",
        "TPL_MIN, TPL_MAX = np.min(TEMPLATE, axis=0), np.max(TEMPLATE, axis=0)\n",
        "MINMAX_TEMPLATE = (TEMPLATE - TPL_MIN) / (TPL_MAX - TPL_MIN)\n",
        "\n",
        "INNER_EYES_AND_BOTTOM_LIP = np.array([39, 42, 57])\n",
        "OUTER_EYES_AND_NOSE = np.array([36, 45, 33])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0_75xONz0MRC"
      },
      "outputs": [],
      "source": [
        "def align_faces(images, landmarks, idx=INNER_EYES_AND_BOTTOM_LIP):\n",
        "    faces = []\n",
        "    for (img, marks) in zip(images, landmarks):\n",
        "        imgDim = img.shape[0]\n",
        "        coords = shape_to_coords(marks)\n",
        "        H = cv2.getAffineTransform(coords[idx], imgDim * MINMAX_TEMPLATE[idx])\n",
        "        warped = cv2.warpAffine(img, H, (imgDim, imgDim))\n",
        "        faces.append(warped)\n",
        "    return faces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChI8odM8YMu6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJAfs5sEIxrd"
      },
      "source": [
        "### Hints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5S7-ag80MRC"
      },
      "source": [
        "The provided function `face_landmarks()` computes the landmarks for a list of cropped faces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kfLRvhHk0MRE"
      },
      "outputs": [],
      "source": [
        "landmarks = face_landmarks(list(faces))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5jItizFr0MRE"
      },
      "outputs": [],
      "source": [
        "new_faces = []\n",
        "for (face,shape) in zip(faces, landmarks):\n",
        "    canvas = face.copy()\n",
        "    coords = shape_to_coords(shape)\n",
        "    for p in coords:\n",
        "        cv2.circle(canvas, (int(p[0]),int(p[1])), 1, (0, 0, 255), -1)\n",
        "    new_faces.append(canvas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "collapsed": true,
        "id": "1a4Sq-uE0MRG",
        "outputId": "f35673f5-4495-43b5-cfdc-c196f86130f7"
      },
      "outputs": [],
      "source": [
        "show_grid(new_faces, figsize=(15,5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vk43JDV10MRG"
      },
      "outputs": [],
      "source": [
        "aligned = align_faces(faces, landmarks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "collapsed": true,
        "id": "YSFdgxSO0MRG",
        "outputId": "e5f6c9dc-7ff9-4333-bb35-8801a9476cec"
      },
      "outputs": [],
      "source": [
        "show_grid(aligned, figsize=(15,5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "collapsed": true,
        "id": "bfTSgga90MRH",
        "outputId": "1dd10db8-eaf2-47e4-8dd6-72e98ba8251d"
      },
      "outputs": [],
      "source": [
        "plt.imshow( np.stack(aligned, axis=3).astype(np.float32).mean(axis=3)/255 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txwta1QKI0YH"
      },
      "source": [
        "### Answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "OzBdWC3RpJdz",
        "outputId": "898512ab-d2fb-4964-ceaa-78e4ce8c606e"
      },
      "outputs": [],
      "source": [
        "landmarks = face_landmarks(list(faces))\n",
        "aligned = align_faces(faces, landmarks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJJSQ9lgpLEV"
      },
      "outputs": [],
      "source": [
        "faces_train, faces_test, labels_train, labels_test = train_test_split(\n",
        "        faces,\n",
        "        labels_one_hot,\n",
        "        test_size=0.3,\n",
        "        random_state=42\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbgV0bsjpNwf"
      },
      "outputs": [],
      "source": [
        "train_data_augmentation = image.ImageDataGenerator(\n",
        "        rescale=1. / 255,\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "train_dataset = train_data_augmentation.flow(\n",
        "        faces_train,\n",
        "        labels_train,\n",
        "        batch_size=15,\n",
        "        ignore_class_split=True,\n",
        "        subset='training'\n",
        "    )\n",
        "\n",
        "test_data_augmentation = image.ImageDataGenerator(rescale=1. / 255)\n",
        "test_dataset = test_data_augmentation.flow(\n",
        "        faces_test,\n",
        "        labels_test,\n",
        "        batch_size=15,\n",
        "        ignore_class_split=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "id": "K_J10NIafdvb",
        "outputId": "ac5821c6-c52f-4667-a753-f4e1544320c5"
      },
      "outputs": [],
      "source": [
        "plot_model_history(cnn_12(train_dataset, test_dataset, 500))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmY1ceqEGZ6t"
      },
      "source": [
        "## 3. Face encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCQgD33c0MRH"
      },
      "source": [
        "The simplest approach to face recognition is to directly classify an unknown face with a convnet trained on your database of tagged people. Seems like a pretty good idea, right? There’s actually a huge problem with that approach. A site like Facebook with billions of users and a trillion photos can’t possibly train such a big convnet. That would take way too long. What you need is a way to extract a few basic measurements from each face, which you can then use to quickly compare the unknown face with your database. For example, you might measure the size of each ear, the spacing between the eyes, the length of the nose, etc. However, it turns out that the measurements that seem obvious to us humans (like eye color) don’t really make sense to a computer looking at individual pixels in an image. Researchers have discovered that the most accurate approach is to let the computer figure out the measurements to collect itself. Deep learning does a better job than humans at figuring out which parts of a face are important to measure.\n",
        "\n",
        "The solution is to train a convnet. But instead of training the network to classify pictures, it is trained to generate 128 measurements for each face. The training process works by looking at 3 face images at a time: the picture of a known person, another picture of the same known person, and a picture of a totally different person. Then, the algorithm looks at the measurements currently generated for each of those three images. It tweaks the neural network slightly to make sure that the measurements generated for the same person are slightly closer, and the measurements for different persons are slightly further apart. After repeating this step millions of times for millions of images of thousands of different people, the convnet learns to generate 128 measurements for each person.\n",
        "\n",
        "<img src=\"https://perso.esiee.fr/~najmanl/FaceRecognition/figures/triplet.png\" style=\"height:400px;\">\n",
        "\n",
        "This process of training a convnet to output face encodings requires a lot of data and computer power. Even with an expensive GPU, it takes about 24 hours of continuous training to get good accuracy. But once the network has been trained, it can generate measurements for any face, even ones it has never seen before! So this step only needs to be done once. Fortunately, the people at [OpenFace](https://cmusatyalab.github.io/openface/) already did this and they published several trained networks which you can directly use. So all you need to do is run your face images through their pre-trained network to get the 128 measurements for each face.\n",
        "\n",
        "<img src=\"https://perso.esiee.fr/~najmanl/FaceRecognition/figures/encoding.png\" style=\"height:300px;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8Dk-ATxJJm6"
      },
      "source": [
        "### Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wo7Hduwu0MRH"
      },
      "source": [
        "Here's what you are required to do for this part of the assignment.\n",
        "\n",
        "- Preprocess the cropped faces by encoding them. You should now have a dataset of cropped and encoded faces.\n",
        "\n",
        "\n",
        "- Train a neural network on the modified dataset. Since the encoded faces are just 128-length vectors, **you don't need a convnet**. Use a regular neural network with a series of fully-connected layers.\n",
        "\n",
        "\n",
        "- Evaluate the performance on the test set, and compare it to the scores obtained with your previously trained convnets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQd40a_iJMgo"
      },
      "source": [
        "### Provided functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "deJ2n5b00MRI"
      },
      "outputs": [],
      "source": [
        "cnn_encoder = dlib.face_recognition_model_v1('models/dlib_face_recognition_resnet_model_v1.dat')\n",
        "\n",
        "def face_encoder(faces):\n",
        "\n",
        "    landmarks = face_landmarks(faces)\n",
        "\n",
        "    if not isinstance(faces, list):\n",
        "        return np.array(cnn_encoder.compute_face_descriptor(faces,landmarks))\n",
        "    else:\n",
        "        return np.array([cnn_encoder.compute_face_descriptor(f,l) for f,l in zip(faces,landmarks)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFRoiWv-JRIz"
      },
      "source": [
        "### Hints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72ujJWBu0MRJ"
      },
      "source": [
        "The provided function `face_encoder()` computes the encodings for a list of cropped faces. Alignment and normalization are handled internally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eIm0yjpR0MRJ"
      },
      "outputs": [],
      "source": [
        "encoded_faces = face_encoder(list(faces))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "Ou94KdUl0MRJ",
        "outputId": "b5c9450e-46b8-41ba-c452-55915f7fc731"
      },
      "outputs": [],
      "source": [
        "plt.plot(encoded_faces[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6umhLN3JYh6"
      },
      "source": [
        "### Answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpBP2KMqtC5Q"
      },
      "outputs": [],
      "source": [
        "encoded_faces = face_encoder(list(faces))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVulY-YFs_PZ"
      },
      "outputs": [],
      "source": [
        "faces_train, faces_test, labels_train, labels_test = train_test_split(\n",
        "        encoded_faces,\n",
        "        labels_one_hot,\n",
        "        test_size=0.3,\n",
        "        random_state=42\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "id": "sQgFvA0pfe_3",
        "outputId": "54a6c663-da13-4f35-87df-46c62bb97f38"
      },
      "outputs": [],
      "source": [
        "cnn_encoded = Sequential()\n",
        "\n",
        "cnn_encoded.add(Dense(256, activation='relu'))\n",
        "cnn_encoded.add(Dense(512, activation='relu'))\n",
        "cnn_encoded.add(Dense(512, activation='relu'))\n",
        "cnn_encoded.add(Dense(256, activation='relu'))\n",
        "cnn_encoded.add(Dense(6, activation='softmax'))\n",
        "\n",
        "cnn_encoded.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = cnn_encoded.fit(faces_train, labels_train, epochs=100, validation_data=(faces_test, labels_test), verbose=0)\n",
        "\n",
        "plot_model_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXI9fWB9GjZ2"
      },
      "source": [
        "## 4. Face recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7x18Hp30MRK"
      },
      "source": [
        "This last step is actually the easiest one in the whole process. All you have to do is find the person in your database of known people who has the closest measurements to some test image. You can do that by using any machine learning classification algorithm, such as neaural network (as you did in the previous section), logistic regression, SVM, nearest neighbours, etc. All you need to do is training a classifier that can take in the measurements from a new test image, and tells which known person is the closest match. Running this classifier must only take milliseconds, so that you can apply it to video sequences.\n",
        "\n",
        "<img src=\"https://perso.esiee.fr/~najmanl/FaceRecognition/figures/test.gif\" style=\"height:300px;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roiwfZk_JhBX"
      },
      "source": [
        "### Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3IRpEzh0MRK"
      },
      "source": [
        "Here's what you are required to do for this part of the assignment.\n",
        "\n",
        "-  Train several classifiers (logistic regression, SVM, kNN, neural network) on the dataset of encoded faces (you can use the package `scikit-learn`).\n",
        "\n",
        "- Evaluate their performance on the test set, in terms of accuracy and speed.\n",
        "\n",
        "- Finally, run your best classifier on the test images and video available in the `test` folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYHj2ZUZ0MRK"
      },
      "source": [
        "### Provided functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "O68ckoUO0MRL"
      },
      "outputs": [],
      "source": [
        "def process_frame(image, mode=\"fast\"):\n",
        "\n",
        "    # face detection\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    if mode == \"fast\":\n",
        "        matches = hog_detector(gray,1)\n",
        "    else:\n",
        "        matches = cnn_detector(gray,1)\n",
        "        matches = [m.rect for m in matches]\n",
        "\n",
        "    for rect in matches:\n",
        "\n",
        "        # face landmarks\n",
        "        landmarks = pose68(gray, rect)\n",
        "\n",
        "        # face encoding\n",
        "        encoding = cnn_encoder.compute_face_descriptor(image, landmarks)\n",
        "\n",
        "        # face classification\n",
        "        label = \"label\"\n",
        "\n",
        "        # draw box\n",
        "        cv2.rectangle(image, (rect.left(), rect.top()), (rect.right(), rect.bottom()), (0, 255, 0), 2)\n",
        "        y = rect.top() - 15 if rect.top() - 15 > 15 else rect.bottom() + 25\n",
        "        cv2.putText(image, label, (rect.left(), y), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMPHcAJI4diD"
      },
      "source": [
        "**Note:** cv2.VideoCapture does not work in Google Colab. You can use https://colab.research.google.com/notebooks/snippets/advanced_outputs.ipynb#scrollTo=2viqYx97hPMi to capture video 'on the fly' with Google Colab. The following function has to be modified accordingly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BeBprnGT0MRL"
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def process_movie(video_name, mode=\"fast\"):\n",
        "\n",
        "    video  = cv2.VideoCapture(video_name)\n",
        "\n",
        "    try:\n",
        "\n",
        "        while True:\n",
        "\n",
        "            # Grab a single frame of video\n",
        "            ret, frame = video.read()\n",
        "\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Resize frame of video for faster processing\n",
        "            frame = cv2.resize(frame, (0, 0), fx=0.5, fy=0.5)\n",
        "\n",
        "            # Quit when the input video file ends or key \"Q\" is pressed\n",
        "            key = cv2.waitKey(1) & 0xFF\n",
        "            if not ret or key == ord(\"q\"):\n",
        "                break\n",
        "\n",
        "            # Process frame\n",
        "            image = process_frame(frame, mode)\n",
        "\n",
        "            # Display the resulting image\n",
        "            # cv2_imshow(image)\n",
        "\n",
        "    finally:\n",
        "        video.release()\n",
        "        cv2.destroyAllWindows()\n",
        "        print(\"Video released\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idhoBkdlJl_E"
      },
      "source": [
        "### Hints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvo3Ojs40MRL"
      },
      "source": [
        "The provided function `process_frame()` detects and encodes all the faces in the input image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jZG-Ebd307i"
      },
      "outputs": [],
      "source": [
        "# !wget https://perso.esiee.fr/~najmanl/FaceRecognition/test.zip\n",
        "# !unzip test.zip\n",
        "download_zip('https://perso.esiee.fr/~najmanl/FaceRecognition/test.zip', 'test.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "daq5vowP0MRL"
      },
      "outputs": [],
      "source": [
        "image = cv2.imread(\"test/example_03.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pYUuFGq-0MRL"
      },
      "outputs": [],
      "source": [
        "processed = process_frame(image.copy())\n",
        "processed = cv2.cvtColor(processed, cv2.COLOR_BGR2RGB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "KYiQ4X530MRM",
        "outputId": "00417527-8610-4188-f552-f0dac049c187"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "plt.imshow(processed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnFPofAs0MRM"
      },
      "source": [
        "The provided function `process_frame()` detects and encodes the faces in the input video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fREoFcuL0MRN",
        "outputId": "b0981357-8a93-4f28-c8fd-fb05ba8a9f72"
      },
      "outputs": [],
      "source": [
        "process_movie(\"test/lunch_scene.mp4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUt43M9a0MRN"
      },
      "source": [
        "The special input `0` can be used to access the webcam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HylqPbF10MRN",
        "outputId": "a64cf077-6e8a-4276-ab1a-9a068c0f446d"
      },
      "outputs": [],
      "source": [
        "process_movie(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx0zQQhoJs4e"
      },
      "source": [
        "### Answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Evm6n0RXtJPk"
      },
      "outputs": [],
      "source": [
        "integer_labels = labels_to_int(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQJRLDRetLmA"
      },
      "outputs": [],
      "source": [
        "faces_train, faces_test, labels_train, labels_test = train_test_split(\n",
        "        encoded_faces,\n",
        "        integer_labels,\n",
        "        test_size=0.3,\n",
        "        random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMo8Nn_MtQBW",
        "outputId": "45b942f0-dadc-4ee6-f780-4f559852975e"
      },
      "outputs": [],
      "source": [
        "logistic_regression_model = LogisticRegression()\n",
        "logistic_regression_model.fit(faces_train, labels_train)\n",
        "\n",
        "training_accuracy = logistic_regression_model.score(faces_train, labels_train)\n",
        "prediction_time = timeit.timeit(lambda: logistic_regression_model.predict(faces_test), number=1)\n",
        "\n",
        "print(\"Training Accuracy:\", training_accuracy)\n",
        "print(\"Prediction Time:\", prediction_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edTZ9Ri9tR1v",
        "outputId": "717d2cae-5017-4a0e-e30c-a46a328be741"
      },
      "outputs": [],
      "source": [
        "svm_model = svm.SVC()\n",
        "svm_model.fit(faces_train, labels_train)\n",
        "\n",
        "training_accuracy = svm_model.score(faces_train, labels_train)\n",
        "prediction_time = timeit.timeit(lambda: svm_model.predict(faces_test), number=1)\n",
        "\n",
        "print(\"Training Accuracy:\", training_accuracy)\n",
        "print(\"Prediction Time:\", prediction_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFPLMnH8tT4O",
        "outputId": "88f0922b-f394-4e75-90f7-ec3d58371750"
      },
      "outputs": [],
      "source": [
        "knn_model = KNeighborsClassifier()\n",
        "knn_model.fit(faces_train, labels_train)\n",
        "\n",
        "training_accuracy = knn_model.score(faces_train, labels_train)\n",
        "prediction_time = timeit.timeit(lambda: knn_model.predict(faces_test), number=1)\n",
        "\n",
        "print(\"Training Accuracy:\", training_accuracy)\n",
        "print(\"Prediction Time:\", prediction_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkLoKM3xtVsA",
        "outputId": "3688e4cb-517a-4645-bd72-a7956f025c9b"
      },
      "outputs": [],
      "source": [
        "neural_network_model = MLPClassifier()\n",
        "neural_network_model.fit(faces_train, labels_train)\n",
        "\n",
        "training_accuracy = neural_network_model.score(faces_train, labels_train)\n",
        "prediction_time = timeit.timeit(lambda: neural_network_model.predict(faces_test), number=1)\n",
        "\n",
        "print(\"Training Accuracy:\", training_accuracy)\n",
        "print(\"Prediction Time:\", prediction_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWbZikJ73egD"
      },
      "outputs": [],
      "source": [
        "def process_frame(image, mode=\"fast\"):\n",
        "    le = LabelEncoder().fit(labels)\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    if mode == \"fast\":\n",
        "        matches = hog_detector(gray,1)\n",
        "    else:\n",
        "        matches = cnn_detector(gray,1)\n",
        "        matches = [m.rect for m in matches]\n",
        "\n",
        "    for rect in matches:\n",
        "        landmarks = pose68(gray, rect)\n",
        "        encoding = cnn_encoder.compute_face_descriptor(image, landmarks)\n",
        "        neural_network_model = MLPClassifier()\n",
        "        neural_network_model.fit(faces_train, labels_train)\n",
        "        predicted_integer_label = neural_network_model.predict([encoding])\n",
        "        label = str(le.inverse_transform([predicted_integer_label]))\n",
        "        cv2.rectangle(image, (rect.left(), rect.top()), (rect.right(), rect.bottom()), (0, 255, 0), 2)\n",
        "        y = rect.top() - 15 if rect.top() - 15 > 15 else rect.bottom() + 25\n",
        "        cv2.putText(image, label, (rect.left(), y), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "VViijXPjtY4p",
        "outputId": "74113492-d3f6-4275-875d-681ba9389547"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread(\"test/example_01.png\")\n",
        "processed = process_frame(img.copy())\n",
        "processed = cv2.cvtColor(processed, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.imshow(processed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo2lrYRVHXtd"
      },
      "source": [
        "## 5. Build a custom dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aothdVBl0MRO"
      },
      "source": [
        "So far, you have used a pre-curated dataset, where somebody did the hard work of gathering and labeling the images for you. Now, you will tackle the problem of recognizing faces of yourselves, friends, family members, colleagues, etc. To accomplish this, you need to gather examples of faces you want to recognize. You can enroll facial pictures via a webcam attached to your computer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7-wf11oKLls"
      },
      "source": [
        "### Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev6q_SyHKKW7"
      },
      "source": [
        "Here's what you are required to do for this part of the assignment.\n",
        "\n",
        "- Use your webcam to enroll face pictures of yourself, your friends, etc. To do so, you need to open the webcam, detect faces in the video stream, and save the captured face images to disk.\n",
        "\n",
        "\n",
        "- Build a dataset of reasonable size: a group of 10-15 people with 50-100 face pictures each, taken in different conditions of light, angle, emotion, etc.\n",
        "\n",
        "\n",
        "- Apply the previously developed pipeline to build your own personalized face recognition system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj20UtshKOJr"
      },
      "source": [
        "### Answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QjIWjK6KKP81",
        "outputId": "574952a1-4a1c-4597-f3c5-6f66cd27a094"
      },
      "outputs": [],
      "source": [
        "custom_data = os.path.join(\"/content\", \"custom_data\")\n",
        "\n",
        "def create_path(dossier = custom_data, prenom : str = 'John', nom : str = 'Doe'):\n",
        "\n",
        "    increment = 0\n",
        "    prenom = prenom.casefold()\n",
        "    nom = nom.casefold()\n",
        "    nom_complet = prenom + \"_\" + nom\n",
        "\n",
        "    dossier_personne = os.path.join(dossier, nom_complet)\n",
        "\n",
        "    if not os.path.exists(dossier_personne):\n",
        "        os.makedirs(dossier_personne)\n",
        "\n",
        "    if len(os.listdir(dossier_personne)) != 0:\n",
        "        increment = len(os.listdir(dossier_personne))\n",
        "\n",
        "    photo = os.path.join(dossier_personne, '{}{}'.format(increment, '.jpeg'))\n",
        "\n",
        "    return photo\n",
        "\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(path_to_directory = custom_data, prenom : str = 'John', nom : str = 'Doe', quality=0.8):\n",
        "\n",
        "  dossier_image = create_path(path_to_directory, prenom, nom)\n",
        "\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(dossier_image, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return dossier_image\n",
        "\n",
        "# take_photo(prenom = \"Robin\", nom = \"Escaffre\")\n",
        "\n",
        "!zip -r 'custom_data.zip' \"custom_data\"\n",
        "!wget https://github.com/robinescaffre/FaceRecognition/raw/main/custom_data.zip\n",
        "!unzip custom_data.zip\n",
        "\n",
        "imagePaths = list_images(\"custom_data\")\n",
        "imagePaths\n",
        "\n",
        "data = np.empty((0, 2), dtype=object)\n",
        "\n",
        "for imagePath in imagePaths:\n",
        "  try:\n",
        "    image = cv2.imread(imagePath)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    face = extract_faces(image, \"cnn\")\n",
        "    label = imagePath.split('/')[1]\n",
        "    face = np.array(face).reshape(128, 128, 3)\n",
        "    data = np.vstack((data, np.array([label, np.array(face)])))\n",
        "  except Exception as e:\n",
        "            print(e)\n",
        "\n",
        "with open(\"face_data\", \"wb\") as file:\n",
        "    pickle.dump(data, file)\n",
        "\n",
        "with open(\"face_data\", \"rb\") as file:\n",
        "    loaded_data = pickle.load(file)\n",
        "\n",
        "labels = np.array([data[0] for data in loaded_data])\n",
        "faces = np.array([data[1] for data in loaded_data])\n",
        "\n",
        "encoded_faces = face_encoder(list(faces))\n",
        "le = LabelEncoder().fit(labels)\n",
        "integer_labels = le.transform(labels)\n",
        "\n",
        "faces_train, faces_test, labels_train, labels_test = train_test_split(\n",
        "        encoded_faces,\n",
        "        integer_labels,\n",
        "        test_size=0.3,\n",
        "        random_state=42\n",
        ")\n",
        "\n",
        "def process_frame(image, mode=\"fast\"):\n",
        "\n",
        "    # face detection\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    if mode == \"fast\":\n",
        "        matches = hog_detector(gray,1)\n",
        "    else:\n",
        "        matches = cnn_detector(gray,1)\n",
        "        matches = [m.rect for m in matches]\n",
        "\n",
        "    for rect in matches:\n",
        "\n",
        "        # face landmarks\n",
        "        landmarks = pose68(gray, rect)\n",
        "\n",
        "        # face encoding\n",
        "        encoding = cnn_encoder.compute_face_descriptor(image, landmarks)\n",
        "\n",
        "        # face classification\n",
        "        neural_network_model = MLPClassifier()\n",
        "        neural_network_model.fit(faces_train, labels_train)\n",
        "        predicted_integer_label = neural_network_model.predict([encoding])\n",
        "        label = str(le.inverse_transform([predicted_integer_label]))\n",
        "\n",
        "        # draw box\n",
        "        cv2.rectangle(image, (rect.left(), rect.top()), (rect.right(), rect.bottom()), (0, 255, 0), 2)\n",
        "        y = rect.top() - 15 if rect.top() - 15 > 15 else rect.bottom() + 25\n",
        "        cv2.putText(image, label, (rect.left(), y), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
        "\n",
        "    return image\n",
        "\n",
        "img0 = cv2.imread(\"sebtest.jpg\")\n",
        "img1 = cv2.imread(\"photoPro.jpg\")\n",
        "img2 = cv2.imread(\"cypriensqueezietest.png\")\n",
        "img3 = cv2.imread(\"logantest.jpg\")\n",
        "\n",
        "processed = process_frame(img0.copy())\n",
        "processed = cv2.cvtColor(processed, cv2.COLOR_BGR2RGB)\n",
        "processed2 = process_frame(img1.copy())\n",
        "processed2 = cv2.cvtColor(processed2, cv2.COLOR_BGR2RGB)\n",
        "processed3 = process_frame(img2.copy())\n",
        "processed3 = cv2.cvtColor(processed3, cv2.COLOR_BGR2RGB)\n",
        "processed4 = process_frame(img3.copy())\n",
        "processed4 = cv2.cvtColor(processed4, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.imshow(processed)\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.imshow(processed2)\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.imshow(processed3)\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.imshow(processed4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2vUz-z20MRO"
      },
      "source": [
        "---\n",
        "## Credits\n",
        "\n",
        "This assignment is based on Adam Geitgey's [post](https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78).\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "y4Wt73GhGH-M",
        "B13bOfafF0Zr",
        "MxS9du380MQw",
        "-62R6B0KIst5",
        "LJAfs5sEIxrd",
        "A8Dk-ATxJJm6",
        "LQd40a_iJMgo",
        "TFRoiWv-JRIz",
        "Wo2lrYRVHXtd"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "nbTranslate": {
      "displayLangs": [
        "*"
      ],
      "hotkey": "alt-t",
      "langInMainMenu": true,
      "sourceLang": "en",
      "targetLang": "fr",
      "useGoogleTranslate": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
